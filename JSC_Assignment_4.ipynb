{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sophie-X31/JSC270-Assignment4/blob/main/JSC_Assignment_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJBu_kcX5P4W"
      },
      "source": [
        "Colab Link: [Link to Collab](https://colab.research.google.com/drive/1ZuIrgB7B7K97vvAgT9wXiWBkouHAC7W2?usp=sharing)\n",
        "\n",
        "Github Link: https://github.com/Sophie-X31/JSC270-Assignment4\n",
        "\n",
        "Original Dataset: https://www.kaggle.com/datasets/thoughtvector/customer-support-on-twitter?resource=download"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erV1RMwh6s69"
      },
      "source": [
        "**Group Members**: Terry Tian, Sophie Xu\n",
        "\n",
        "Preamble: The work for this project was split evenly, where each member contributed to both performing the analytics as well as writing the report and preparing the presentation. The joint effort meant both members participated in all segments of the assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NU5cTAUXRT3s"
      },
      "outputs": [],
      "source": [
        "## Import Statements\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "import plotly.graph_objs as go\n",
        "from nltk.stem.porter import *\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.tokenize import regexp_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, roc_curve\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report, roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from plotly.subplots import make_subplots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kc4J2e-i5LC9"
      },
      "source": [
        "# Part 1: Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdsY-uzU2pRg"
      },
      "source": [
        "A. Initial Observation\n",
        "\n",
        "The proportion of observations in the training set can be summarized as \n",
        "\n",
        "1.   Positive = 18046/41157 $\\approx$ 43.85%\n",
        "2.   Neutral = 15398/41157 $\\approx$ 37.41%\n",
        "3.   Negative = 7713/41157 $\\approx$ 18.74%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkh24niNUCkw",
        "outputId": "384f9d28-265c-45bf-d945-fc555fb91d73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 41157 entries, 1 to 41157\n",
            "Data columns (total 6 columns):\n",
            " #   Column         Non-Null Count  Dtype \n",
            "---  ------         --------------  ----- \n",
            " 0   UserName       41157 non-null  object\n",
            " 1   ScreenName     41157 non-null  object\n",
            " 2   Location       32567 non-null  object\n",
            " 3   TweetAt        41157 non-null  object\n",
            " 4   OriginalTweet  41157 non-null  object\n",
            " 5   Sentiment      41157 non-null  int64 \n",
            "dtypes: int64(1), object(5)\n",
            "memory usage: 1.9+ MB\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2    18046\n",
              "0    15398\n",
              "1     7713\n",
              "Name: Sentiment, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ],
      "source": [
        "## Read Dataset\n",
        "covid_test = pd.read_csv('https://raw.githubusercontent.com/Sophie-X31/JSC270-Assignment4/main/Corona_NLP_test.csv', header=None)\n",
        "covid_train = pd.read_csv('https://raw.githubusercontent.com/Sophie-X31/JSC270-Assignment4/main/Corona_NLP_train.csv', header=None, encoding='latin-1')\n",
        "\n",
        "## Data Wrangling\n",
        "def wrangle_df(df):\n",
        "  df.columns = ['UserName', 'ScreenName', 'Location', 'TweetAt', 'OriginalTweet', 'Sentiment']\n",
        "  df.drop(index=df.index[0], axis=0, inplace=True)\n",
        "  df['Sentiment'] = df['Sentiment'].replace(['Extremely Positive', 'Positive'], 2)\n",
        "  df['Sentiment'] = df['Sentiment'].replace('Neutral', 1)\n",
        "  df['Sentiment'] = df['Sentiment'].replace(['Extremely Negative', 'Negative'], 0) \n",
        "wrangle_df(covid_train)\n",
        "wrangle_df(covid_test)\n",
        "\n",
        "## Count Proportion\n",
        "covid_train.info()\n",
        "covid_train['Sentiment'].value_counts(dropna = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WVFu5EO6Eos"
      },
      "source": [
        "B. Tokenize Tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pkicym9YZqb"
      },
      "outputs": [],
      "source": [
        "## Tokenize\n",
        "def tokenize(df):\n",
        "  df['OriginalTweet'] = df['OriginalTweet'].apply(str.strip) # Remove excess whitespace\n",
        "  nltk.download('punkt')\n",
        "  df['tokens'] = df['OriginalTweet'].apply(nltk.word_tokenize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ax6Y-35iFyI"
      },
      "source": [
        "C. Remove Hyperlink"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OO_MjSjQiHdB"
      },
      "outputs": [],
      "source": [
        "## Remove URL\n",
        "def remove_url(df):\n",
        "  tokens_no_url = []\n",
        "  for row in df['tokens']:\n",
        "    tokens_no_url.append([re.sub('^http','', w) for w in row])\n",
        "  df['tokens'] = tokens_no_url"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4nI-5u7hR4Z"
      },
      "source": [
        "D. Remove Special Characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-QhCrothZw5"
      },
      "outputs": [],
      "source": [
        "## Remove punctuations\n",
        "def remove_punc(df):\n",
        "  tokens_no_punct = []\n",
        "  for row in df['tokens']:\n",
        "    tokens_no_punct.append([re.sub('[^\\w\\s]','', w) for w in row])\n",
        "  df['tokens'] = tokens_no_punct\n",
        "  df['OriginalTweet'] = df['OriginalTweet'].apply(str.lower) # Convert to lowercase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDbqXhQjhrnn"
      },
      "source": [
        "E. Stem Tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0eA_lsJhxdo"
      },
      "outputs": [],
      "source": [
        "## Stemming\n",
        "def stem(df):  \n",
        "  stemmer = PorterStemmer()\n",
        "  stemmed_tokens = []\n",
        "  for row in df['tokens']:\n",
        "    stemmed_tokens.append([stemmer.stem(w) for w in row])\n",
        "  df['tokens'] = stemmed_tokens\n",
        "\n",
        "## Lemmatization\n",
        "def lemm(df):\n",
        "  nltk.download('wordnet') \n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  lem_tokens = []\n",
        "  for row in df['tokens']:\n",
        "    lem_tokens.append([lemmatizer.lemmatize(w) for w in row])\n",
        "  df['tokens'] = lem_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzl72Hx6hrtr"
      },
      "source": [
        "F. Remove Stop Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80AYmC0bh6hD"
      },
      "outputs": [],
      "source": [
        "## Remove stopwords\n",
        "def remove_stopw(df):\n",
        "  nltk.download('stopwords')\n",
        "  sw = stopwords.words('english')[:100]\n",
        "  tokens_no_sw = []\n",
        "  for row in df['tokens']:\n",
        "    tokens_no_sw.append([w for w in row if w not in sw])\n",
        "  df['tokens'] = tokens_no_sw\n",
        "\n",
        "  no_blanks = [] # Remove blank tokens\n",
        "  for row in df['tokens']:\n",
        "    no_blanks.append([w for w in row if w != ''])\n",
        "  df['tokens'] = no_blanks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYh5enJUjT90"
      },
      "source": [
        "G. Vectorize Token Collection with Count Vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mATbamLXg8xb",
        "outputId": "a51da841-980c-433c-948c-c1ad86fbb3b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "## Put everything together\n",
        "def tokenize_df(df: pd.DataFrame, lem: bool = False) -> None:\n",
        "  tokenize(df)\n",
        "  remove_url(df)\n",
        "  remove_punc(df)\n",
        "  if (lem):\n",
        "    lemm(df)\n",
        "  else:\n",
        "    stem(df)\n",
        "  remove_stopw(df)\n",
        "\n",
        "train1, test1 = covid_train.copy(), covid_test.copy()\n",
        "tokenize_df(train1)\n",
        "x_train, y_train = train1['tokens'].to_numpy(), train1['Sentiment'].to_numpy()\n",
        "tokenize_df(test1)\n",
        "x_test, y_test = test1['tokens'].to_numpy(), test1['Sentiment'].to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlnuoFVh42IM"
      },
      "outputs": [],
      "source": [
        "## Count Vectorizer\n",
        "def override_func(doc):\n",
        "  return doc\n",
        "\n",
        "count_vec = CountVectorizer(\n",
        "    analyzer='word',\n",
        "    tokenizer= override_func,\n",
        "    preprocessor= override_func,\n",
        "    token_pattern= None,\n",
        "    max_features = 1000)\n",
        "counts_train = count_vec.fit_transform(x_train)\n",
        "counts_test = count_vec.fit_transform(x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWgJ_lFszCfy"
      },
      "source": [
        "H. Fit Naive Bayes Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zBZf_LZmzMxx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a03355cd-e8e0-484b-f0bd-c9e13f7cb964"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy is: 0.6827271181087057\n",
            "\n",
            "Testing Accuracy is: 0.41495523959978936\n"
          ]
        }
      ],
      "source": [
        "x_train = counts_train.toarray()\n",
        "x_test = counts_test.toarray()\n",
        " \n",
        "## Fit Model\n",
        "model = MultinomialNB()\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "train_pred = model.predict(x_train)\n",
        "test_pred = model.predict(x_test)\n",
        "\n",
        "train_acc = accuracy_score(y_train, train_pred)\n",
        "test_acc = accuracy_score(y_test, test_pred)\n",
        "\n",
        "print(\"Training Accuracy is: \" + str(train_acc) + \"\\n\")\n",
        "print(\"Testing Accuracy is: \" + str(test_acc))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This function take a while to run\n",
        "import pprint\n",
        "services = [0, 1, 2]\n",
        "\n",
        "\n",
        "def concatenate_tokens(df, service):\n",
        "  service_df = df[df['Sentiment'] == service]\n",
        "  word_bank = ' '\n",
        "  for index in service_df.index:\n",
        "    lst = service_df['tokens'][index]\n",
        "    for i in range(len(lst)):\n",
        "      word_bank = word_bank + lst[i] + ' '\n",
        "  return word_bank\n",
        "\n",
        "## Helper: All tokens of a company\n",
        "def full_tokens(df, service):\n",
        "  word_bank = concatenate_tokens(df, service)\n",
        "  return word_bank.split()\n",
        "\n",
        "# Finding Most Common Words\n",
        "def most_common_all(df, k):\n",
        "  all = dict()\n",
        "  for service in services:\n",
        "    fd = nltk.FreqDist(full_tokens(df, service))\n",
        "    words_tuple = fd.most_common(k)\n",
        "    # fd.tabulate(50)\n",
        "    all[service] = words_tuple\n",
        "  return all\n",
        "\n",
        "most_common = most_common_all(train1, 5)"
      ],
      "metadata": {
        "id": "3OBVrfAHL2y8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "pprint(most_common)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yc6-Cfo1dNob",
        "outputId": "b0c7355c-f9c0-4571-99d1-9cee0eb8ce15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: [('s', 10117),\n",
            "     ('coronaviru', 6737),\n",
            "     ('covid19', 4610),\n",
            "     ('price', 4347),\n",
            "     ('food', 3639)],\n",
            " 1: [('s', 6287),\n",
            "     ('coronaviru', 3812),\n",
            "     ('covid19', 2567),\n",
            "     ('store', 1588),\n",
            "     ('supermarket', 1442)],\n",
            " 2: [('s', 12911),\n",
            "     ('coronaviru', 7512),\n",
            "     ('covid19', 5684),\n",
            "     ('store', 3918),\n",
            "     ('thi', 3783)]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wd1FokJrAT8t"
      },
      "source": [
        "I. Fit ROC Curve\n",
        "\n",
        "It would not be appropriate to fit a ROC curve in this scenario because this dataset is imbalanced. Moreover, in this situation of having three labels, we can only plot ROC curves for one class versus the rest, which gives limited information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mjl6ZqCqBbzb"
      },
      "source": [
        "J. Vectorize Token Collection with TF_IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pzqj_ofiBjN_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7f482c8-af2e-42af-8217-5fc1c5d18383"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy is: 0.6667395582768423\n",
            "\n",
            "Testing Accuracy is: 0.432596103212217\n"
          ]
        }
      ],
      "source": [
        "## TF-IDF Vectorize\n",
        "tfidf = TfidfTransformer()\n",
        "x_tf_train = tfidf.fit_transform(counts_train).toarray()\n",
        "x_tf_test = tfidf.fit_transform(counts_test).toarray()\n",
        "\n",
        "## Fit Model\n",
        "model = MultinomialNB()\n",
        "model.fit(x_tf_train, y_train)\n",
        "\n",
        "train_pred = model.predict(x_tf_train)\n",
        "test_pred = model.predict(x_tf_test)\n",
        "\n",
        "train_acc = accuracy_score(y_train, train_pred)\n",
        "test_acc = accuracy_score(y_test, test_pred)\n",
        "\n",
        "print(\"Training Accuracy is: \" + str(train_acc) + \"\\n\")\n",
        "print(\"Testing Accuracy is: \" + str(test_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZ3q8FRCEN7N"
      },
      "source": [
        "K. Use Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3lhAD2mEQOl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "895540d4-68d5-4108-8f13-161717365542"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy is: 0.6405714702237773\n",
            "\n",
            "Testing Accuracy is: 0.43417588204318064\n"
          ]
        }
      ],
      "source": [
        "## Tokenize Tweets\n",
        "train2, test2 = covid_train.copy(), covid_test.copy()\n",
        "tokenize_df(train2, True)\n",
        "x_train2, y_train2 = train2['tokens'].to_numpy(), train2['Sentiment'].to_numpy()\n",
        "tokenize_df(test2, True)\n",
        "x_test2, y_test2 = test2['tokens'].to_numpy(), test2['Sentiment'].to_numpy()\n",
        "\n",
        "## Vectorize with TF-IDF\n",
        "counts_train2 = count_vec.fit_transform(x_train2)\n",
        "counts_test2 = count_vec.fit_transform(x_test2)\n",
        "x_tf_train2 = tfidf.fit_transform(counts_train2).toarray()\n",
        "x_tf_test2 = tfidf.fit_transform(counts_test2).toarray()\n",
        "\n",
        "## Fit Model\n",
        "model = MultinomialNB()\n",
        "model.fit(x_tf_train2, y_train)\n",
        "\n",
        "train_pred = model.predict(x_tf_train2)\n",
        "test_pred = model.predict(x_tf_test2)\n",
        "\n",
        "train_acc = accuracy_score(y_train, train_pred)\n",
        "test_acc = accuracy_score(y_test, test_pred)\n",
        "\n",
        "print(\"Training Accuracy is: \" + str(train_acc) + \"\\n\")\n",
        "print(\"Testing Accuracy is: \" + str(test_acc))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0K-7_YxCGbcd"
      },
      "source": [
        "BONUS: \n",
        "\n",
        "The Naive Bayes model is generative because, unlike a logistic regression where we can only predict whether an input has the label, the Bayes model allows us to predict the label for multiple classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F00SuCWmLff4"
      },
      "source": [
        "# Part 2: NLP using Twitter API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BX_c6vHck_E"
      },
      "source": [
        "Step 1: Data Wrangling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3QtVaTlabvcm"
      },
      "outputs": [],
      "source": [
        "## Concatenate DataFrame\n",
        "apple = pd.read_csv('https://raw.githubusercontent.com/Sophie-X31/JSC270-Assignment4/main/AppleSupport.csv', header=None, encoding='latin-1')\n",
        "air = pd.read_csv('https://raw.githubusercontent.com/Sophie-X31/JSC270-Assignment4/main/AirAsiaSupport.csv', header=None, encoding='latin-1')\n",
        "rbc = pd.read_csv('https://raw.githubusercontent.com/Sophie-X31/JSC270-Assignment4/main/AskRBC.csv', header=None, encoding='latin-1')\n",
        "mcdon = pd.read_csv('https://raw.githubusercontent.com/Sophie-X31/JSC270-Assignment4/main/McDonalds.csv', header=None, encoding='latin-1')\n",
        "nike = pd.read_csv('https://raw.githubusercontent.com/Sophie-X31/JSC270-Assignment4/main/NikeSupport.csv', header=None, encoding='latin-1')\n",
        "spotify = pd.read_csv('https://raw.githubusercontent.com/Sophie-X31/JSC270-Assignment4/main/SpotifyCares.csv', header=None, encoding='latin-1')\n",
        "walmart = pd.read_csv('https://raw.githubusercontent.com/Sophie-X31/JSC270-Assignment4/main/Walmart.csv', header=None, encoding='latin-1')\n",
        "\n",
        "def drop_row(df):\n",
        "  df.columns = ['tweet_id', 'author_id', 'inbound', 'created_at', 'text', 'response_tweet_id', 'in_response_to_tweet_id']\n",
        "  df.drop(index=df.index[0], axis=0, inplace=True)\n",
        "frames = [apple, air, rbc, mcdon, nike, spotify, walmart]\n",
        "for name in frames:\n",
        "  drop_row(name)\n",
        "df = pd.concat(frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xK1ATtUoeqN_"
      },
      "outputs": [],
      "source": [
        "## Overview of Dataset\n",
        "print(df.info())\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sa6PfYPPcrSZ"
      },
      "source": [
        "Step 2: Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nm1DhOj8b3gj"
      },
      "outputs": [],
      "source": [
        "## Tokenize\n",
        "def tokenize2(df):\n",
        "  df['text'] = df['text'].apply(str.strip)\n",
        "  df['text'] = df['text'].apply(str.lower)\n",
        "  nltk.download('punkt')\n",
        "  df['tokens'] = df['text'].apply(nltk.word_tokenize)\n",
        "\n",
        "def remove_punc2(df):\n",
        "  tokens_no_punct = []\n",
        "  for row in df['tokens']:\n",
        "    tokens_no_punct.append([re.sub('[^\\w\\s]','', w) for w in row])\n",
        "  df['tokens'] = tokens_no_punct\n",
        "\n",
        "def tokenize_df2(df) -> None:\n",
        "  tokenize2(df)\n",
        "  remove_url(df)\n",
        "  remove_punc2(df)\n",
        "  remove_stopw(df)\n",
        "  lemm(df)\n",
        "\n",
        "## Vectorize\n",
        "tokenize_df2(df)\n",
        "x, y = df['tokens'].to_numpy(), df['author_id'].to_numpy()\n",
        "count_x = count_vec.fit_transform(x)\n",
        "X = TfidfTransformer().fit_transform(count_x).toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WD09dgLgcuGT"
      },
      "outputs": [],
      "source": [
        "## WordCloud Image\n",
        "def concatenate_tokens(df, service):\n",
        "  service_df = df[df['author_id'] == service]\n",
        "  word_bank = ' '\n",
        "  for index in service_df.index:\n",
        "    lst = service_df['tokens'][index]\n",
        "    for i in range(len(lst)):\n",
        "      word_bank = word_bank + lst[i] + ' '\n",
        "  return word_bank\n",
        "\n",
        "def plot_WordCloud(df, service):\n",
        "  wordcloud = WordCloud(width = 800, height = 800,\n",
        "                        background_color ='white',\n",
        "                        stopwords = set(STOPWORDS),\n",
        "                        min_font_size = 10).generate(concatenate_tokens(df, service))                     \n",
        "  plt.figure(figsize = (8, 8), facecolor = None)\n",
        "  plt.imshow(wordcloud)\n",
        "  plt.axis(\"off\")\n",
        "  plt.tight_layout(pad = 0)\n",
        "  plt.show()\n",
        "\n",
        "services = ['AppleSupport', 'AirAsiaSupport', 'AskRBC', 'McDonalds', 'NikeSupport', 'SpotifyCares', 'Walmart']\n",
        "for s in services:\n",
        "  plot_WordCloud(df, s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDAXYUSZgqOI"
      },
      "outputs": [],
      "source": [
        "## Helper: All tokens of a company\n",
        "def full_tokens(df, service):\n",
        "  word_bank = concatenate_tokens(df, service)\n",
        "  return word_bank.split()\n",
        "\n",
        "## Frequency Distribution for Top 50 Most Common Words of Each Company\n",
        "def most_common_all(df, k):\n",
        "  all = dict()\n",
        "  for service in services:\n",
        "    fd = nltk.FreqDist(full_tokens(df, service))\n",
        "    words_tuple = fd.most_common(k)\n",
        "    # fd.tabulate(50)\n",
        "    all[service] = words_tuple\n",
        "  return all\n",
        "most_common = most_common_all(df, 50)\n",
        "\n",
        "## Helper: Only words\n",
        "def set_words_all(most_common):\n",
        "  all = dict()\n",
        "  for service in services:\n",
        "    coll = set()\n",
        "    words = most_common[service]\n",
        "    for i in range(len(words)):\n",
        "      coll.add(words[i][0])\n",
        "    all[service] = coll\n",
        "  return all\n",
        "set_most_common = set_words_all(most_common)\n",
        "\n",
        "## Helper: List of all words without duplicates\n",
        "def union_list(set_most_common):\n",
        "  union_set = []\n",
        "  for service in services:\n",
        "    words_set = set_most_common[service]\n",
        "    union_set.extend(words_set)\n",
        "  return union_set\n",
        "union_lst = union_list(set_most_common)\n",
        "\n",
        "## Condense into pd DataFrame\n",
        "most_common_words = pd.DataFrame(columns = ['company', 'words', 'frequency'])\n",
        "for service in services:\n",
        "  word_list = most_common[service]\n",
        "  for i in range(len(word_list)):\n",
        "    most_common_words = most_common_words.append({'company' : service, 'words' : word_list[i][0], 'frequency' : word_list[i][1]}, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yu0OQVpfwhLz"
      },
      "outputs": [],
      "source": [
        "## Find duplicated common words\n",
        "def find_duplicates(union_lst, k):\n",
        "  duplicates = []\n",
        "  fd = nltk.FreqDist(union_lst)\n",
        "  distr = fd.most_common(k)\n",
        "  for i in range(len(distr)):\n",
        "    if distr[i][1] > 1:\n",
        "      duplicates.append(distr[i][0])\n",
        "  return (distr, duplicates)\n",
        "distr = find_duplicates(union_lst, 100)[0]\n",
        "duplicates = find_duplicates(union_lst, 100)[1]\n",
        "\n",
        "## Helper: Check highest frequency for the given word\n",
        "def compare_freq(word, freq, service, most_common_words):\n",
        "  other_companies = services.copy()\n",
        "  other_companies.remove(service)\n",
        "  for ser in other_companies:\n",
        "    word_freq_lst = most_common_words[most_common_words['company'] == ser]\n",
        "    for index in range(len(word_freq_lst.index)):\n",
        "      if (word == word_freq_lst['words'].iloc[index]) and (freq < word_freq_lst['frequency'].iloc[index]):\n",
        "        return False\n",
        "  return True\n",
        "\n",
        "## Find the top k unique most frequent words\n",
        "def unique(most_common_words, service, k):\n",
        "  unique = pd.DataFrame(columns = ['words', 'frequency'])\n",
        "  word_freq_lst = most_common_words[most_common_words['company'] == service]\n",
        "  count, index = 0, 0\n",
        "  while count < k:\n",
        "    word = word_freq_lst['words'].iloc[index]\n",
        "    freq = word_freq_lst['frequency'].iloc[index]\n",
        "    if word in duplicates:\n",
        "      if compare_freq(word, freq, service, most_common_words):\n",
        "        unique = unique.append({'words' : word, 'frequency' : freq}, ignore_index=True)\n",
        "        count = count + 1\n",
        "    else:\n",
        "      unique = unique.append({'words' : word, 'frequency' : freq}, ignore_index=True)\n",
        "      count = count + 1\n",
        "    index = index + 1\n",
        "  return unique\n",
        "unique(most_common_words, services[1], 10) # Index 1 for AirAsiaSupport\n",
        "\n",
        "## Find the top k most commonly used words\n",
        "def top_duplicated_common_words(distr, k):\n",
        "  dist = pd.DataFrame(columns = ['words', 'frequency'])\n",
        "  count, index = 0, 0\n",
        "  while count < k:\n",
        "    group = distr[index]\n",
        "    word = group[0]\n",
        "    freq = group[1]\n",
        "    if len(word) > 1:\n",
        "      dist = dist.append({'words' : word, 'frequency' : freq}, ignore_index=True)\n",
        "      count = count + 1\n",
        "    index = index + 1\n",
        "  return dist\n",
        "top_duplicated_common_words(distr, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2FzelxKlVnO"
      },
      "outputs": [],
      "source": [
        "## Sentiment Assignment\n",
        "nltk.download('vader_lexicon')\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "df['sentiment'] = [sia.polarity_scores(txt).get('compound') for txt in df['text']]\n",
        "\n",
        "## Condense into pd DataFrame\n",
        "avg_sentiment = pd.DataFrame(columns = ['company', 'average_polarity_score'])\n",
        "for i in range(len(services)):\n",
        "  avg = df[df['author_id'] == services[i]]['sentiment'].mean()\n",
        "  avg_sentiment = avg_sentiment.append({'company' : services[i], 'average_polarity_score' : avg}, ignore_index=True)\n",
        "avg_sentiment\n",
        "\n",
        "## Graph Results\n",
        "ax = sns.barplot(x=\"company\", y=\"average_polarity_score\", data=avg_sentiment, palette='crest')\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\n",
        "for i in ax.containers:\n",
        "    ax.bar_label(i,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1z3E7T2cvDf"
      },
      "source": [
        "Step 3: Development and Evaluation of Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PioX7gRkcdq_"
      },
      "outputs": [],
      "source": [
        "## Split Dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
        "\n",
        "## Model (Testing)\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(accuracy_score(y_test,y_pred))\n",
        "\n",
        "## Model (Training)\n",
        "model2 = MultinomialNB()\n",
        "model2.fit(X_train, y_train)\n",
        "y_pred2 = model.predict(X_train)\n",
        "print(accuracy_score(y_train,y_pred2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNzRpRv1Ta6r"
      },
      "outputs": [],
      "source": [
        "# Classification report\n",
        "cr_dict = classification_report(y_test, y_pred, output_dict=True)\n",
        "# Define the columns of the table\n",
        "columns = ['precision', 'recall', 'f1-score', 'support']\n",
        "\n",
        "# Create a figure and axes object\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "# Remove the axes spines and ticks\n",
        "ax.axis('off')\n",
        "\n",
        "# Add a title\n",
        "ax.set_title('Classification Report')\n",
        "\n",
        "# Define the table format\n",
        "table_data = []\n",
        "\n",
        "for label, scores in cr_dict.items():\n",
        "    if label == 'accuracy':\n",
        "        continue\n",
        "    row_data = [label] + [scores[col] for col in columns]\n",
        "    table_data.append(row_data)\n",
        "\n",
        "# Add the table to the plot\n",
        "columns = ['Company', 'precision', 'recall', 'f1-score', 'support']\n",
        "table = ax.table(cellText=table_data, colLabels=columns, loc='center')\n",
        "\n",
        "# Set the color of the column labels to red\n",
        "for j in range(len(columns)):\n",
        "    cell = table.get_celld()[(0, j)]\n",
        "    cell.set_facecolor('#ede3daff')\n",
        "\n",
        "# Adjust te figure size and padding\n",
        "fig.set_size_inches(8, 4)  # Change the figure size to 8x4 inches\n",
        "table.set_fontsize(14)  # Set the font size of the table\n",
        "table.scale(1.5, 1.5)  # Scale the table by 1.5x in both directions\n",
        "ax.axis('off')  # Turn off the axis\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwhmBo_0ThI9"
      },
      "outputs": [],
      "source": [
        "## Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred, labels=model.classes_)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n",
        "disp.plot(xticks_rotation=30)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95vhpxjSTjsD"
      },
      "outputs": [],
      "source": [
        "## ROC and AUC\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# Fit a naive Bayes classifier on the training data\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Binarize the labels\n",
        "y_test_bin = label_binarize(y_test, classes=np.unique(y))\n",
        "\n",
        "# Compute the predicted scores for each class\n",
        "y_score = model.predict_proba(X_test)\n",
        "\n",
        "# Compute the ROC curve and AUC score for each class\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "for i in range(len(np.unique(y))):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "# Set the number of rows and columns for the plot grid\n",
        "num_rows = int(np.ceil(len(np.unique(y))/3))\n",
        "num_cols = 3\n",
        "\n",
        "# Set the figure size\n",
        "plt.figure(figsize=(15, 9))\n",
        "\n",
        "# Plot the ROC curve for each class on a separate subplot\n",
        "lst = np.unique(y)\n",
        "\n",
        "for i in range(len(np.unique(y))):\n",
        "    c = lst[i]\n",
        "    plt.subplot(num_rows, num_cols, i+1)\n",
        "    plt.plot(fpr[i], tpr[i], color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc[i])\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC curve for class '+ c)\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    \n",
        "# Adjust the layout of the subplots\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}